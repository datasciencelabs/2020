---
title: 'Lab 9: Machine Learning II'
output:
  html_document:
    toc: true
---

This lab is split into three sections, each of which covers a different set of machine learning topics and operates independently of the other two parts. For convenience, each section loads all of the R packages needed to run the code in that section. This results in some packages being re-loaded if you knit the Rmarkdown file or run all of the chunks, which is generally not a good practice. 

Before we begin, make sure that all of the following packages are installed on your computer: 

- `caret` 
- `dslabs` 
- `GGally` 
- `MASS` 
- `mvtnorm` 
- `pROC` 
- `randomForest` 
- `tidyverse` 
- `tree`


# 1. kNN, LDA, and QDA for Binary Classification

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(mvtnorm)
library(caret)
library(MASS)
library(pROC)
```

In this section, we simulate a dataset that has 200 observations from Class 0 and 200 observations from Class 1. The class labels are stored in the variable Y. There are also two predictor variables, X1 and X2. When an observation is from Class 0, X1 and X2 are drawn from a bivariate normal distribution with mean $\mu_0 = \left( \begin{array}{c} 3 \\ 2 \end{array} \right)$ and covariance matrix $\Sigma = \left( \begin{array}{cc} 1 & 0.5 \\ 0.5 & 1 \end{array} \right)$. When an observation is from Class 1, X1 and X2 are drawn from a bivariate normal distribution with mean $\mu_1 = \left( \begin{array}{c} 4 \\ 4 \end{array} \right)$ and the same covariance matrix $\Sigma = \left( \begin{array}{cc} 1 & 0.5 \\ 0.5 & 1 \end{array} \right)$. 

In slightly less technical terms, this means that when an observation is from Class 0, X1 is normally distributed with mean 3 and standard deviation 1; X2 is normally distributed with mean 2 and standard deviation 1; and the two predictors have a moderately positive correlation. When an observation is from Class 1, X1 and X2 are still normally distributed with the same correlation structure as in Class 0, but their means are 4 instead. 

```{r}
# Means for two classes
mu0 = c(3, 2)
mu1 = c(4, 4)
# Shared covariance matrix.
Sigma = matrix(c(1, 0.5, 0.5, 1), nrow = 2)

# Simulate 200 observations from each class and create vector of labels
set.seed(1)
sim_dat = data.frame(Y = factor(rep(c(0, 1), each = 200)), 
                     rbind(mvrnorm(200, mu0, Sigma), 
                           mvrnorm(200, mu1, Sigma)))

# Take a peek at the simulated data
head(sim_dat)
```


1.1. Use the `createDataPartition` function from the `caret` package to split the data into training and test sets (50% training and 50% test). Be sure to set a random seed so that your code is reproducible (the solutions use `set.seed(4)`). Visualize the training observations by making a scatterplot of X2 against X1 and coloring the points by Y. Comment on what you see. 


1.2. Use the k-Nearest Neighbors (kNN) algorithm with k=1 neighbors to model Y based on X1 and X2 in the training data. You can use the `knn3` function from the `caret` package. Save the output from `knn3` as `fit_knn1`. Then, run kNN with k=15 neighbors and save your model as `fit_knn15`. For each model, obtain the predicted probabilities of belonging in Class 1 for the observations in the test set. 


1.3. Use the `lda` and `qda` functions from the `MASS` package to fit linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA) models to the training data. Save your models as `fit_lda` and `fit_qda`, respectively. Both X1 and X2 should be included as predictors. For each model, obtain the predicted probabilities of belonging in Class 1 for the observations in the test set. 


1.4. Using the `pROC` package and the predicted probabilities from the four models that you fit in Q1.2-3, plot and interpret the receiver operating characteristic (ROC) curves based on the test set. Also, report the area under the curve (AUC) for each of the four models. 


1.5. The code below plots the test set data, as well as decision boundaries calculated from the `fit_knn`, `fit_knn15`, `fit_lda`, and `fit_qda` models that you built in Q1.2-3. To run, remove the `eval = FALSE` option from the code chunk header. Discuss how the assumptions of the models and the data-generating process are reflected in the shapes of the decision boundaries. 

```{r, eval = FALSE}
# Create a grid of X1 and X2 values that span the space of the test set
grid = expand.grid(seq(min(sim_test_set$X1), 
                       max(sim_test_set$X1), length=200), 
                   seq(min(sim_test_set$X2), 
                       max(sim_test_set$X2), length=200))
colnames(grid) = c("X1", "X2")

# Predict class labels on all values in the grid for each model
boundary_df = data.frame(
  grid, 
  knn1 = predict(fit_knn1, newdata = grid, type="class"), 
  knn15 = predict(fit_knn15, newdata = grid, type="class"), 
  lda = predict(fit_lda, newdata = grid)$class, 
  qda = predict(fit_qda, newdata = grid)$class)

# Scatterplot of test set observations
# Overlay contour plot to draw decision boundaries
boundary_df %>% gather(model, pred, c(knn1, knn15, lda, qda)) %>%
  ggplot() + 
  geom_point(aes(x = X1, y = X2, shape = Y), data = sim_test_set) + 
  stat_contour(aes(x = X1, y = X2, z = as.numeric(pred), color = model), 
               lwd = 1, breaks=c(1, 2, 3)) + 
  scale_shape_manual(values = c(16, 3)) +
  scale_color_discrete(name = "Model", 
                       labels = c("kNN (k=1)", "kNN (k=15)", "LDA", "QDA"))
```


# 2. Regression and Decision Trees for Continuous Outcomes

In this part of the lab, we will predict infant birth weight using the `birthwt` dataset, available in the `MASS` package. This dataset of 189 observations was  collected at the Baystate Medical Center in Springfield, MA during 1986. It includes the following variables: 

- `low`: indicator of birth weight less than 2.5 kg (0 = more than 2.5 kg, 1 = less than 2.5 kg).
- `age`: mother's age in years.
- `lwt`: mother's weight in pounds at last menstrual period.
- `race`: mother's race (1 = white, 2 = black, 3 = other).
- `smoke`: smoking status during pregnancy (0 = nonsmoker, 1 = smoker).
- `ptl`: number of previous premature labors.
- `ht`: history of hypertension (0 = no history, 1 = history of hypertension).
- `ui`: presence of uterine irritability (0 = no presence, 1 = presence of uterine irritability).
- `ftv`: number of physician visits during the first trimester.
- `bwt`: birth weight in grams.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(tree)

library(MASS)
data(birthwt)
```

Since we are interested in predicting birth weight, we drop the `low` indicator from the dataset. We also convert `race`, `smoke`, `ht`, and `ui` to factors, because these variables are categorical and not numeric/continuous. 

Comment: Both the `dplyr` and `MASS` packages export a function called `select`, which is why the `dplyr::` is necessary in `dplyr::select`. This makes it clear to R that we want to use the `select` function from `dplyr`. 

```{r}
birthwt = birthwt %>% dplyr::select(-low) %>%
  mutate(race = factor(race), 
         smoke = factor(smoke), 
         ht = factor(ht), 
         ui = factor(ui))
```

We use `createDataPartition` to split the birth weight data into equally-sized training and test sets. 

```{r}
set.seed(9)
birthwt_index_train = createDataPartition(y = birthwt$bwt, 
                                  times = 1, p = 0.5, list = FALSE)
birthwt_train_set = slice(birthwt, birthwt_index_train)
birthwt_test_set = slice(birthwt, -birthwt_index_train)
```


2.1. Based on the training set, make some plots to help you assess the relationship between our outcome of interest, `bwt`, and each of the other eight variables in the dataset. Which variables seem like good predictors of birth weight? 


2.2. Fit a linear regression model with `lm` that predicts `bwt` using all of the other variables in the training data. Print out the summary information and identify variables with significant coefficients. 


2.3. Fit a regression tree that predicts `bwt` using all of the other variables in the training data. You can use the `tree` function from the `tree` package. Make a plot that visualizes the tree, and compare the variables used to construct the tree with the variables that were significant in the linear model from Q2.2. 


2.4. Use the `cv.tree` function to determine a reasonable tree size. Be sure to set a random seed so that your code is reproducible (the solutions use `set.seed(16)`). Prune your tree from Q2.3 to have this "best" size, using the `best` argument in the `prune.tree` function. Visualize the pruned tree and compare the variables used to construct it with the variables used to construct the tree from Q2.3. 


2.5. For each of the three models from Q2.2-4, calculate birth weight predictions for the observations in the test set. Compare model performance in terms of the test mean-squared error (MSE). 


# 3. Decision Trees, Bagging, and Random Forests for Multi-Class Outcomes

In the final section of this lab, we will use gene expression data to classify tissue samples. The data can be loaded from the `dslabs` package by calling `data(tissue_gene_expression)`. `tissuesGeneExpression` is a list with two elements: 

- `x`: Numeric matrix with 189 rows and 500 columns. Each column contains gene expression measurements for a different gene. 
- `y`: Factor vector of length 189 that records tissue type labels (cerebellum, colon, endometrium, hippocampus, kidney, liver, or placenta) . 

The original data (accessible in the `tissuesGeneExpression` package) records gene expression for 22,215 genes. 

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(GGally)
library(tree)
library(randomForest)

library(dslabs)
data(tissue_gene_expression)
```

We will only use a random sample of 100 genes to predict tissue type. 

```{r}
set.seed(25)
tiss_ge = data.frame(y = tissue_gene_expression$y, 
                     tissue_gene_expression$x[,sample(500, 100)])
```

As usual, we split the data into training and test sets, each with about 50% of the data. 

```{r}
set.seed(36)
tiss_ge_index_train = createDataPartition(y = tiss_ge$y, 
                                  times = 1, p = 0.5, list = FALSE)
tiss_ge_train_set = slice(tiss_ge, tiss_ge_index_train)
tiss_ge_test_set = slice(tiss_ge, -tiss_ge_index_train)
```


3.1. Below, you will find some plots and tables of the training set designed to help you develop some intuition for the data. Describe what you see. 

This is a frequency table for the tissue types in the training data. 

```{r}
table(tiss_ge_train_set$y)
```

It is difficult to make visualizations for all 100 genes in the dataset, so let's randomly sample six to focus on. 

```{r}
set.seed(49)
genes6 = sample(names(tiss_ge)[-1], 6)
genes6
```

Here are histograms of the gene expression distributions of the six genes.  

```{r}
tiss_ge_train_set %>% 
  gather(gene, expression, all_of(genes6))  %>% 
  ggplot(aes(x = expression)) + 
  geom_histogram() + 
  facet_wrap(~ gene) + 
  xlab(NULL) + ylab(NULL)
```

The boxplots below plot gene expression against tissue type for the six genes. Note that setting `scales = 'free_y'` allows the y-axis to vary from plot to plot, so they are not on the same scale. 

```{r}
tiss_ge_train_set %>% 
  gather(gene, expression, all_of(genes6))  %>% 
  ggplot(aes(x = y, y = expression)) + 
  geom_boxplot() + 
  facet_wrap(~ gene, scales = 'free_y') + 
  xlab(NULL) + ylab(NULL) + 
  scale_x_discrete(labels = str_to_title(unique(tiss_ge_train_set$y))) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```

The `ggcorr` function from the `GGally` package makes pretty correlation matrix plots. Each tiny square in this plot represents the correlation between a pair of genes (out of the entire set of 100 genes). Red indicates positive correlation (close to 1), blue indicates negative correlation (close to -1), and white indicates no correlation (close to 0). 

```{r}
ggcorr(tiss_ge_train_set[,-1], hjust = 1, size = 1.5, layout.exp = 10)
```


3.2. Using the `tree` function from the `tree` package and all of the training set gene expression data, build a decision tree to classify the tissue types. Get the predicted class labels for the test set data, report the test accuracy, and comment on the test confusion matrix. 


3.3. Fit a bagging (bootstrap aggregation) model to the training data by running `randomForest` from the `randomForest` package with the `mtry` parameter set to the number of predictors (`mtry = 100`). Be sure to set a random seed so that your code is reproducible (the solutions use `set.seed(64)`). Get the predicted class labels for the test set data and report the test accuracy. 


3.4. Now, build a random forest model with the `mtry` parameter set to the square root of the number of predictors. Also, set `importance = TRUE` so that the importance of the predictors is assessed. You will need the variable importance information for Q3.5. Be sure to set a random seed so that your code is reproducible (the solutions use `set.seed(81)`). Get the predicted class labels for the test set data and report the test accuracy. 


3.5. Run the `importance` function on your random forest model from Q3.4 to extract variable importance measures for each of the tissue types. Find the five most important genes for classifying kidney tissues by ordering the Gini index measures. Compare these five genes with the genes that were used to construct the classification tree in Q3.2.

Optional: Extract the five most important genes for each of the seven tissues, and compare these results with the genes that were used to construct the classification tree in Q3.2.
